{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4c8e853",
   "metadata": {},
   "source": [
    "# Some code\n",
    "This file converts World Bank netcdf data into csv format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f65984a",
   "metadata": {},
   "source": [
    "## Prepare\n",
    "install the following packages if not installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ca069ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install xarray\n",
    "# %pip install ttictoc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1bc168",
   "metadata": {},
   "source": [
    "Set working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b51c6dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acae3e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Current working directory is: C:\\Users\\user\\Documents\n"
     ]
    }
   ],
   "source": [
    "print(\"The Current working directory is: {0}\".format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bbc2282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory has been changed to: C:\\Users\\user\\Desktop\\RA file\\EnergyAndWeather\\wb_data\n"
     ]
    }
   ],
   "source": [
    "os.chdir(r\"C:\\Users\\user\\Desktop\\RA file\\EnergyAndWeather\\wb_data\") # change here\n",
    "print(\"Working directory has been changed to: {0}\".format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae593b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is system\n",
      " Volume Serial Number is 4C26-1A36\n",
      "\n",
      " Directory of C:\\Users\\user\\Desktop\\RA file\\EnergyAndWeather\\wb_data\n",
      "\n",
      "08/12/2023  12:08 PM    <DIR>          .\n",
      "08/12/2023  12:04 PM    <DIR>          ..\n",
      "08/11/2023  08:18 PM       625,719,410 final_wb_data.dta\n",
      "08/10/2023  11:26 PM        74,720,056 five_day_rainfall.nc\n",
      "08/10/2023  11:59 PM        74,719,805 frost_day.nc\n",
      "08/11/2023  08:32 PM            23,859 gen_csv_wb_data.ipynb\n",
      "08/11/2023  12:24 AM        74,720,998 hi35_day.nc\n",
      "08/11/2023  01:11 AM        74,719,805 hot_day.nc\n",
      "08/11/2023  01:11 AM        74,719,805 ice_day.nc\n",
      "08/10/2023  10:11 PM        74,720,045 max_dry_day.nc\n",
      "08/10/2023  10:20 PM        74,720,045 max_wet_day.nc\n",
      "08/11/2023  08:37 PM             3,277 merge_wb_data.do\n",
      "08/10/2023  11:12 PM        74,720,018 one_day_rainfall.nc\n",
      "08/12/2023  12:08 PM        74,719,989 rainfall.nc\n",
      "08/11/2023  10:54 PM    <DIR>          temp_csv_dta_files\n",
      "08/12/2023  12:07 PM        74,719,869 temper.nc\n",
      "08/10/2023  11:48 PM        74,720,056 wet_rainfall.nc\n",
      "              14 File(s)  1,447,667,037 bytes\n",
      "               3 Dir(s)  55,294,414,848 bytes free\n"
     ]
    }
   ],
   "source": [
    "# netcdf files are stored in this directory.\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be731a63",
   "metadata": {},
   "source": [
    "The csv file generated will be big. We define a function to report file size later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbebdf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    " \n",
    " \n",
    "def get_file_size(file):\n",
    "    stat = os.stat(file)\n",
    "    size = stat.st_size\n",
    "    return size\n",
    "\n",
    " \n",
    "def convert_bytes(size, unit=None):\n",
    "    if unit == \"KB\":\n",
    "        return print('csv size: ' + str(round(size / 1024, 3)) + ' Kilobytes')\n",
    "    elif unit == \"MB\":\n",
    "        return print('csv size: ' + str(round(size / (1024 * 1024), 3)) + ' Megabytes')\n",
    "    elif unit == \"GB\":\n",
    "        return print('csv size: ' + str(round(size / (1024 * 1024 * 1024), 3)) + ' Gigabytes')\n",
    "    else:\n",
    "        return print('csv size: ' + str(size) + ' bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09701760",
   "metadata": {},
   "source": [
    "Import time to report time elapsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33be360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from ttictoc import tic,toc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77b0bf2",
   "metadata": {},
   "source": [
    "## Convert netcdf into csv format\n",
    "Use `max_dry_day.nc` as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44c2d975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:                     (year: 71, lat: 360, lon: 720, bnds: 2)\n",
      "Coordinates:\n",
      "  * year                        (year) float64 1.95e+03 1.951e+03 ... 2.02e+03\n",
      "  * lat                         (lat) float64 -89.75 -89.25 ... 89.25 89.75\n",
      "  * lon                         (lon) float64 -179.8 -179.2 ... 179.2 179.8\n",
      "  * bnds                        (bnds) int32 0 1\n",
      "Data variables:\n",
      "    timeseries-cdd-annual-mean  (year, lat, lon) float32 ...\n",
      "    lon_bnds                    (lon, bnds) float64 ...\n",
      "    lat_bnds                    (lat, bnds) float64 ...\n",
      "Attributes:\n",
      "    comment:              Processed ERA5 data by WB; RAW-data from ECMWF: htt...\n",
      "    NCO:                  netCDF Operators version 4.7.9 (Homepage = http://n...\n",
      "    history:              Fri Nov 13 19:17:20 2020: ncks -4 --ppc default=7 e...\n",
      "    NETCDF_COMPRESSION:   NCO: Precision-preserving compression to netCDF4/HD...\n",
      "    Conventions:          CF-1.6\n",
      "    CONVERSION_DATE:      Fri Nov 13 19:17:11 MST 2020\n",
      "    CONVERSION_PLATFORM:  Linux r8i5n27 4.12.14-95.51-default #1 SMP Fri Apr ...\n",
      "    NETCDF_VERSION:       4.7.3\n",
      "    NETCDF_CONVERSION:    CISL RDA: Conversion from ECMWF GRIB1 data to netCDF4.\n",
      "    DATA_SOURCE:          ECMWF: https://cds.climate.copernicus.eu, Copernicu...\n"
     ]
    }
   ],
   "source": [
    "filename=\"max_dry_day.nc\"\n",
    "ds = xr.open_dataset(filename)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fef1650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            timeseries-cdd-annual-mean  lon_bnds  lat_bnds\n",
      "year   lat    lon     bnds                                                \n",
      "1950.0 -89.75 -179.75 0                           87.0    -180.0     -90.0\n",
      "                      1                           87.0    -179.5     -89.5\n",
      "              -179.25 0                           87.0    -179.5     -90.0\n",
      "                      1                           87.0    -179.0     -89.5\n",
      "              -178.75 0                           87.0    -179.0     -90.0\n",
      "...                                                ...       ...       ...\n",
      "2020.0  89.75  178.75 1                           30.0     179.0      90.0\n",
      "               179.25 0                           30.0     179.0      89.5\n",
      "                      1                           30.0     179.5      90.0\n",
      "               179.75 0                           30.0     179.5      89.5\n",
      "                      1                           30.0     180.0      90.0\n",
      "\n",
      "[36806400 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df = ds.to_dataframe()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b17ed00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            max_dry_day  lon_bnds  lat_bnds\n",
      "year   lat    lon     bnds                                 \n",
      "1950.0 -89.75 -179.75 0            87.0    -180.0     -90.0\n",
      "                      1            87.0    -179.5     -89.5\n",
      "              -179.25 0            87.0    -179.5     -90.0\n",
      "                      1            87.0    -179.0     -89.5\n",
      "              -178.75 0            87.0    -179.0     -90.0\n",
      "...                                 ...       ...       ...\n",
      "2020.0  89.75  178.75 1            30.0     179.0      90.0\n",
      "               179.25 0            30.0     179.0      89.5\n",
      "                      1            30.0     179.5      90.0\n",
      "               179.75 0            30.0     179.5      89.5\n",
      "                      1            30.0     180.0      90.0\n",
      "\n",
      "[36806400 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# rename key variables\n",
    "df = df.rename(columns = {'timeseries-cdd-annual-mean':filename[:-3]}, inplace = False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17757fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 199.33929199998965\n"
     ]
    }
   ],
   "source": [
    "# save as csv\n",
    "tic()\n",
    "\n",
    "df.to_csv(filename[:-3]+'.csv',index='False',mode='w+')\n",
    "\n",
    "\n",
    "elapsed = toc()\n",
    "print('Elapsed time:',elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "707eecff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv size: 1.355 Gigabytes\n"
     ]
    }
   ],
   "source": [
    "# report csv file size\n",
    "csvfile = filename[:-3]+'.csv'\n",
    "size = get_file_size(csvfile)\n",
    "convert_bytes(size, \"GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d2bd63",
   "metadata": {},
   "source": [
    "## Drop unnecessary variables \n",
    "\n",
    "After we delete (`bnds`,`lon_buds`,`lat_bnds`) and drop duplicates from the csv file, the file size will be smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b8d17ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            year    lat     lon  bnds  max_dry_day  lon_bnds  lat_bnds\n",
      "0         1950.0 -89.75 -179.75     0         87.0    -180.0     -90.0\n",
      "1         1950.0 -89.75 -179.75     1         87.0    -179.5     -89.5\n",
      "2         1950.0 -89.75 -179.25     0         87.0    -179.5     -90.0\n",
      "3         1950.0 -89.75 -179.25     1         87.0    -179.0     -89.5\n",
      "4         1950.0 -89.75 -178.75     0         87.0    -179.0     -90.0\n",
      "...          ...    ...     ...   ...          ...       ...       ...\n",
      "36806395  2020.0  89.75  178.75     1         30.0     179.0      90.0\n",
      "36806396  2020.0  89.75  179.25     0         30.0     179.0      89.5\n",
      "36806397  2020.0  89.75  179.25     1         30.0     179.5      90.0\n",
      "36806398  2020.0  89.75  179.75     0         30.0     179.5      89.5\n",
      "36806399  2020.0  89.75  179.75     1         30.0     180.0      90.0\n",
      "\n",
      "[36806400 rows x 7 columns]\n",
      "Elapsed time: 62.83943759999238\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tic()\n",
    "\n",
    "df = pd.read_csv(filename[:-3]+'.csv')\n",
    "print(df)\n",
    "\n",
    "print('Elapsed time:',toc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b554b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            year    lat     lon  max_dry_day\n",
      "0         1950.0 -89.75 -179.75         87.0\n",
      "2         1950.0 -89.75 -179.25         87.0\n",
      "4         1950.0 -89.75 -178.75         87.0\n",
      "6         1950.0 -89.75 -178.25         87.0\n",
      "8         1950.0 -89.75 -177.75         87.0\n",
      "...          ...    ...     ...          ...\n",
      "36806390  2020.0  89.75  177.75         30.0\n",
      "36806392  2020.0  89.75  178.25         30.0\n",
      "36806394  2020.0  89.75  178.75         30.0\n",
      "36806396  2020.0  89.75  179.25         30.0\n",
      "36806398  2020.0  89.75  179.75         30.0\n",
      "\n",
      "[18403200 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# drop boundary coordinates\n",
    "df = df.drop(columns=['bnds','lon_bnds','lat_bnds'])\n",
    "# Only keep one duplicate row\n",
    "df = df.drop_duplicates()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9131fc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 171.23807910000323\n"
     ]
    }
   ],
   "source": [
    "# save as csv and replace the original file\n",
    "tic()\n",
    "\n",
    "df.to_csv(filename[:-3]+'.csv',index='False',mode='w+')\n",
    "\n",
    "\n",
    "elapsed = toc()\n",
    "print('Elapsed time:',elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55e94d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv size: 613.526 Megabytes\n"
     ]
    }
   ],
   "source": [
    "# report csv file size\n",
    "csvfile = filename[:-3]+'.csv'\n",
    "size = get_file_size(csvfile)\n",
    "convert_bytes(size, \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62af9767",
   "metadata": {},
   "source": [
    "## Repeat the process above for remaining files\n",
    "`max_wet_day.nc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43fb1c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 452.9719657000096\n",
      "csv size: 605.734 Megabytes\n"
     ]
    }
   ],
   "source": [
    "tic()\n",
    "\n",
    "\n",
    "filename=\"max_wet_day.nc\"\n",
    "ds = xr.open_dataset(filename)\n",
    "df = ds.to_dataframe()\n",
    "df = df.rename(columns = {'timeseries-cwd-annual-mean':filename[:-3]}, inplace = False)\n",
    "df.to_csv(filename[:-3]+'.csv',index='False',mode='w+') # save as csv\n",
    "df = pd.read_csv(filename[:-3]+'.csv')\n",
    "df = df.drop(columns=['bnds','lon_bnds','lat_bnds']) # drop boundary coordinates\n",
    "df = df.drop_duplicates() # Only keep one duplicate row\n",
    "df.to_csv(filename[:-3]+'.csv',index='False',mode='w+')\n",
    "\n",
    "print('Elapsed time:',toc())\n",
    "\n",
    "csvfile = filename[:-3]+'.csv'\n",
    "size = get_file_size(csvfile)\n",
    "convert_bytes(size, \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e371ff9",
   "metadata": {},
   "source": [
    "`one_day_rainfall.nc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60b46421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 461.3584544000041\n",
      "csv size: 627.368 Megabytes\n"
     ]
    }
   ],
   "source": [
    "tic()\n",
    "\n",
    "\n",
    "filename=\"one_day_rainfall.nc\"\n",
    "ds = xr.open_dataset(filename)\n",
    "df = ds.to_dataframe()\n",
    "df = df.rename(columns = {'timeseries-rx1day-annual-mean':filename[:-3]}, inplace = False)\n",
    "df.to_csv(filename[:-3]+'.csv',index='False',mode='w+') # save as csv\n",
    "df = pd.read_csv(filename[:-3]+'.csv')\n",
    "df = df.drop(columns=['bnds','lon_bnds','lat_bnds']) # drop boundary coordinates\n",
    "df = df.drop_duplicates() # Only keep one duplicate row\n",
    "df.to_csv(filename[:-3]+'.csv',index='False',mode='w+')\n",
    "\n",
    "print('Elapsed time:',toc())\n",
    "\n",
    "csvfile = filename[:-3]+'.csv'\n",
    "size = get_file_size(csvfile)\n",
    "convert_bytes(size, \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aa59af",
   "metadata": {},
   "source": [
    "`five_day_rainfall.nc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e47bf5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 449.6015490999853\n",
      "csv size: 631.942 Megabytes\n"
     ]
    }
   ],
   "source": [
    "tic()\n",
    "\n",
    "\n",
    "filename=\"five_day_rainfall.nc\"\n",
    "ds = xr.open_dataset(filename)\n",
    "df = ds.to_dataframe()\n",
    "df = df.rename(columns = {'timeseries-rx5day-annual-mean':filename[:-3]}, inplace = False)\n",
    "df.to_csv(filename[:-3]+'.csv',index='False',mode='w+') # save as csv\n",
    "df = pd.read_csv(filename[:-3]+'.csv')\n",
    "df = df.drop(columns=['bnds','lon_bnds','lat_bnds']) # drop boundary coordinates\n",
    "df = df.drop_duplicates() # Only keep one duplicate row\n",
    "df.to_csv(filename[:-3]+'.csv',index='False',mode='w+')\n",
    "\n",
    "print('Elapsed time:',toc())\n",
    "\n",
    "csvfile = filename[:-3]+'.csv'\n",
    "size = get_file_size(csvfile)\n",
    "convert_bytes(size, \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c600dcf3",
   "metadata": {},
   "source": [
    "`wet_rainfall.nc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b2d1488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 516.0723609999986\n",
      "csv size: 641.541 Megabytes\n"
     ]
    }
   ],
   "source": [
    "tic()\n",
    "\n",
    "\n",
    "filename=\"wet_rainfall.nc\"\n",
    "ds = xr.open_dataset(filename)\n",
    "df = ds.to_dataframe()\n",
    "df = df.rename(columns = {'timeseries-r95ptot-annual-mean':filename[:-3]}, inplace = False)\n",
    "df.to_csv(filename[:-3]+'.csv',index='False',mode='w+') # save as csv\n",
    "df = pd.read_csv(filename[:-3]+'.csv')\n",
    "df = df.drop(columns=['bnds','lon_bnds','lat_bnds']) # drop boundary coordinates\n",
    "df = df.drop_duplicates() # Only keep one duplicate row\n",
    "df.to_csv(filename[:-3]+'.csv',index='False',mode='w+')\n",
    "\n",
    "print('Elapsed time:',toc())\n",
    "\n",
    "csvfile = filename[:-3]+'.csv'\n",
    "size = get_file_size(csvfile)\n",
    "convert_bytes(size, \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbb2ee9",
   "metadata": {},
   "source": [
    "`frost_day.nc`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ce25160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 1363.9586674999737\n",
      "csv size: 665.092 Megabytes\n"
     ]
    }
   ],
   "source": [
    "tic()\n",
    "\n",
    "\n",
    "filename=\"frost_day.nc\"\n",
    "ds = xr.open_dataset(filename)\n",
    "df = ds.to_dataframe()\n",
    "df = df.rename(columns = {'timeseries-fd-annual-mean':filename[:-3]}, inplace = False)\n",
    "df.to_csv(filename[:-3]+'.csv',index='False',mode='w+') # save as csv\n",
    "df = pd.read_csv(filename[:-3]+'.csv')\n",
    "df = df.drop(columns=['bnds','lon_bnds','lat_bnds']) # drop boundary coordinates\n",
    "df = df.drop_duplicates() # Only keep one duplicate row\n",
    "df.to_csv(filename[:-3]+'.csv',index='False',mode='w+')\n",
    "\n",
    "print('Elapsed time:',toc())\n",
    "\n",
    "csvfile = filename[:-3]+'.csv'\n",
    "size = get_file_size(csvfile)\n",
    "convert_bytes(size, \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bd6684",
   "metadata": {},
   "source": [
    "`hi35_day.nc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0369c4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 1232.7362042000168\n",
      "csv size: 649.37 Megabytes\n"
     ]
    }
   ],
   "source": [
    "tic()\n",
    "\n",
    "\n",
    "filename=\"hi35_day.nc\"\n",
    "ds = xr.open_dataset(filename)\n",
    "df = ds.to_dataframe()\n",
    "df = df.rename(columns = {'timeseries-hi35-annual-mean':filename[:-3]}, inplace = False)\n",
    "df.to_csv(filename[:-3]+'.csv',index='False',mode='w+') # save as csv\n",
    "df = pd.read_csv(filename[:-3]+'.csv')\n",
    "df = df.drop(columns=['bnds','lon_bnds','lat_bnds']) # drop boundary coordinates\n",
    "df = df.drop_duplicates() # Only keep one duplicate row\n",
    "df.to_csv(filename[:-3]+'.csv',index='False',mode='w+')\n",
    "\n",
    "print('Elapsed time:',toc())\n",
    "\n",
    "csvfile = filename[:-3]+'.csv'\n",
    "size = get_file_size(csvfile)\n",
    "convert_bytes(size, \"MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c559d274",
   "metadata": {},
   "source": [
    "`hot_day.nc`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e70b172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 1112.8847228999948\n",
      "csv size: 650.616 Megabytes\n"
     ]
    }
   ],
   "source": [
    "tic()\n",
    "\n",
    "\n",
    "filename=\"hot_day.nc\"\n",
    "ds = xr.open_dataset(filename)\n",
    "df = ds.to_dataframe()\n",
    "df = df.rename(columns = {'timeseries-hd35-annual-mean':filename[:-3]}, inplace = False)\n",
    "df.to_csv(filename[:-3]+'.csv',index='False',mode='w+') # save as csv\n",
    "df = pd.read_csv(filename[:-3]+'.csv')\n",
    "df = df.drop(columns=['bnds','lon_bnds','lat_bnds']) # drop boundary coordinates\n",
    "df = df.drop_duplicates() # Only keep one duplicate row\n",
    "df.to_csv(filename[:-3]+'.csv',index='False',mode='w+')\n",
    "\n",
    "print('Elapsed time:',toc())\n",
    "\n",
    "csvfile = filename[:-3]+'.csv'\n",
    "size = get_file_size(csvfile)\n",
    "convert_bytes(size, \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a3fcc2",
   "metadata": {},
   "source": [
    "`ice_day.nc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b124ebae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 1215.6614080000145\n",
      "csv size: 663.359 Megabytes\n"
     ]
    }
   ],
   "source": [
    "tic()\n",
    "\n",
    "\n",
    "filename=\"ice_day.nc\"\n",
    "ds = xr.open_dataset(filename)\n",
    "df = ds.to_dataframe()\n",
    "df = df.rename(columns = {'timeseries-id-annual-mean':filename[:-3]}, inplace = False)\n",
    "df.to_csv(filename[:-3]+'.csv',index='False',mode='w+') # save as csv\n",
    "df = pd.read_csv(filename[:-3]+'.csv')\n",
    "df = df.drop(columns=['bnds','lon_bnds','lat_bnds']) # drop boundary coordinates\n",
    "df = df.drop_duplicates() # Only keep one duplicate row\n",
    "df.to_csv(filename[:-3]+'.csv',index='False',mode='w+')\n",
    "\n",
    "print('Elapsed time:',toc())\n",
    "\n",
    "csvfile = filename[:-3]+'.csv'\n",
    "size = get_file_size(csvfile)\n",
    "convert_bytes(size, \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004c3f3d",
   "metadata": {},
   "source": [
    "`temper.nc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6481d5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 559.996739299997\n",
      "csv size: 630.808 Megabytes\n"
     ]
    }
   ],
   "source": [
    "tic()\n",
    "\n",
    "\n",
    "filename=\"mean_temper.nc\"\n",
    "ds = xr.open_dataset(filename)\n",
    "df = ds.to_dataframe()\n",
    "df = df.rename(columns = {'timeseries-tas-annual-mean':filename[:-3]}, inplace = False)\n",
    "df.to_csv(filename[:-3]+'.csv',index='False',mode='w+') # save as csv\n",
    "df = pd.read_csv(filename[:-3]+'.csv')\n",
    "df = df.drop(columns=['bnds','lon_bnds','lat_bnds']) # drop boundary coordinates\n",
    "df = df.drop_duplicates() # Only keep one duplicate row\n",
    "df.to_csv(filename[:-3]+'.csv',index='False',mode='w+')\n",
    "\n",
    "print('Elapsed time:',toc())\n",
    "\n",
    "csvfile = filename[:-3]+'.csv'\n",
    "size = get_file_size(csvfile)\n",
    "convert_bytes(size, \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35546c76",
   "metadata": {},
   "source": [
    "`rainfall.nc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb4fb7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 574.1694542999903\n",
      "csv size: 651.394 Megabytes\n"
     ]
    }
   ],
   "source": [
    "tic()\n",
    "\n",
    "\n",
    "filename=\"rainfall.nc\"\n",
    "ds = xr.open_dataset(filename)\n",
    "df = ds.to_dataframe()\n",
    "df = df.rename(columns = {'timeseries-pr-annual-mean':filename[:-3]}, inplace = False)\n",
    "df.to_csv(filename[:-3]+'.csv',index='False',mode='w+') # save as csv\n",
    "df = pd.read_csv(filename[:-3]+'.csv')\n",
    "df = df.drop(columns=['bnds','lon_bnds','lat_bnds']) # drop boundary coordinates\n",
    "df = df.drop_duplicates() # Only keep one duplicate row\n",
    "df.to_csv(filename[:-3]+'.csv',index='False',mode='w+')\n",
    "\n",
    "print('Elapsed time:',toc())\n",
    "\n",
    "csvfile = filename[:-3]+'.csv'\n",
    "size = get_file_size(csvfile)\n",
    "convert_bytes(size, \"MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
